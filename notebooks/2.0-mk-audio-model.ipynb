{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.feature_extraction import EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.feature_extraction.settings import from_columns\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "ROOT_PATH = Path(\"..\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ids, labels, and train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ROOT_PATH / \"data/raw/metadata.csv\")\n",
    "# svc - song vs call ids\n",
    "# filter ids -> <20s, quality A & B\n",
    "# svc ids -> only rows that have call or song (not both)\n",
    "filter_ids = pd.read_json(ROOT_PATH / \"data/raw/filter_ids.json\").squeeze()\n",
    "svc_ids = pd.read_json(ROOT_PATH / \"data/raw/song_vs_call.json\").squeeze()\n",
    "svc_df = df.loc[df.id.isin(svc_ids)].copy()\n",
    "# set index to id\n",
    "svc_df.set_index(\"id\", inplace=True)\n",
    "\n",
    "with open(ROOT_PATH / \"data/processed/svc_split.json\") as svc_split_file:\n",
    "    svc_split = json.load(svc_split_file)\n",
    "    train_ids = svc_split[\"train_ids\"]\n",
    "    test_ids = svc_split[\"test_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add response variable\n",
    "type_col = svc_df.type.str.lower().str.replace(\" \", \"\").str.split(\",\")\n",
    "filtered_type_col = type_col.apply(lambda l: set(l) - {\"call\", \"song\"})\n",
    "svc_df[\"pred\"] = type_col.apply(lambda l: \"call\" in l).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build y train-test\n",
    "# indexing all (svc_df and y_df) by id\n",
    "y_df = svc_df[\"pred\"]\n",
    "y_train, y_test = (\n",
    "    y_df[y_df.index.isin(train_ids)].squeeze(),\n",
    "    y_df[y_df.index.isin(test_ids)].squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Audio to Timeseries, Run High-pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply butter filter\n",
    "def highpass_filter(audio, sr):\n",
    "    # butter_coeff_b, butter_coeff_a = signal.butter(3, 1000, btype='highpass', fs=sr) # numerator and denominator\n",
    "    # butter_audio = signal.lfilter(butter_coeff_b, butter_coeff_a, audio)\n",
    "    # return butter_audio\n",
    "    return signal.lfilter(*signal.butter(3, 1000, btype=\"highpass\", fs=sr), audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack an mp3 or wav into df of timeseries values\n",
    "def unpack_audio(id):\n",
    "    try:\n",
    "        audio_path = ROOT_PATH / (\"data/raw/recordings/\" + str(id) + \".mp3\")\n",
    "        # load mp3 as audio timeseries arr\n",
    "        timeseries, sr = librosa.load(audio_path)\n",
    "    except FileNotFoundError:\n",
    "        audio_path = ROOT_PATH / (\"data/raw/recordings/\" + str(id) + \".wav\")\n",
    "        timeseries, sr = librosa.load(audio_path)\n",
    "\n",
    "    # high-pass filter on audio timeseries\n",
    "    timeseries_filt = highpass_filter(timeseries, sr)\n",
    "\n",
    "    df = pd.DataFrame(timeseries_filt, columns=[\"val\"])\n",
    "    df.reset_index(inplace=True)\n",
    "    df[\"id\"] = id  # fill col with id\n",
    "    df = df.reindex(columns=[\"id\", \"index\", \"val\"])\n",
    "    df.columns = [\"id\", \"time\", \"val\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# unpack_audio(svc_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to calculate\n",
    "# features can be found here: https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.feature_calculators.fft_aggregated\n",
    "manual_fc_params = {\n",
    "    \"abs_energy\": None,\n",
    "    \"fft_aggregated\": [{\"aggtype\": \"centroid\"}, {\"aggtype\": \"kurtosis\"}],\n",
    "    \"root_mean_square\": None,\n",
    "    \"spkt_welch_density\": [{\"coeff\": 2}, {\"coeff\": 5}, {\"coeff\": 8}],\n",
    "}\n",
    "\n",
    "selected_fc_params = {\n",
    "    \"standard_deviation\": None,\n",
    "    \"variance\": None,\n",
    "    \"root_mean_square\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def featurize_audio(id, fc_params):\n",
    "    return extract_features(\n",
    "        unpack_audio(id),\n",
    "        column_id=\"id\",\n",
    "        column_sort=\"time\",\n",
    "        default_fc_parameters=fc_params,\n",
    "        disable_progressbar=True,\n",
    "        # we impute = remove all NaN features automatically\n",
    "        impute_function=impute,\n",
    "        # turn off parallelization\n",
    "        n_jobs=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurize dataset\n",
    "# returns df of all combined\n",
    "def featurize_set(ids, fc_params=None):\n",
    "    if fc_params is None:\n",
    "        fc_params = EfficientFCParameters()\n",
    "    X_df = pd.DataFrame()\n",
    "    for id in tqdm(ids):\n",
    "        X_df = pd.concat([X_df, featurize_audio(id, fc_params)])\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate features\n",
    "# takes 1 hr\n",
    "# X_df = featurize_set(svc_ids, manual_fc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save calculated features\n",
    "# X_df.to_json(ROOT_PATH / f\"data/processed/audio_features_manual.json\", indent=2, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "X_df = pd.read_json(\n",
    "    path_or_buf=ROOT_PATH / \"data/processed/audio_features.json\", orient=\"columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features\n",
    "full_df = X_df.copy()\n",
    "full_df[\"label\"] = y_df\n",
    "full_df\n",
    "\n",
    "\n",
    "def plot_audio_byfeat(ix, iy, x_title, y_title, x_lim=None, y_lim=None):\n",
    "    plt.scatter(\n",
    "        ix.loc[full_df[\"label\"] == 1],\n",
    "        iy.loc[full_df[\"label\"] == 1],\n",
    "        marker=\"x\",\n",
    "        color=\"C0\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        ix.loc[full_df[\"label\"] == 0],\n",
    "        iy.loc[full_df[\"label\"] == 0],\n",
    "        marker=\"+\",\n",
    "        color=\"C3\",\n",
    "    )\n",
    "    if x_lim is not None:\n",
    "        plt.xlim([0, x_lim])\n",
    "    if y_lim is not None:\n",
    "        plt.ylim([0, y_lim])\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.legend([\"call\", \"song\"])\n",
    "\n",
    "\n",
    "spectral_density = full_df[\"val__spkt_welch_density__coeff_8\"]\n",
    "fft_centroid = full_df['val__fft_aggregated__aggtype_\"centroid\"']\n",
    "fft_kurtosis = full_df['val__fft_aggregated__aggtype_\"kurtosis\"']\n",
    "rms = full_df[\"val__root_mean_square\"]\n",
    "abs_energy = full_df[\"val__abs_energy\"]\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plot_audio_byfeat(\n",
    "    spectral_density, fft_centroid, \"spectral density\", \"fft centroid\", 0.01, 150000\n",
    ")\n",
    "plt.subplot(2, 3, 2)\n",
    "plot_audio_byfeat(spectral_density, rms, \"spectral density\", \"rms\", 0.01, 0.15)\n",
    "plt.subplot(2, 3, 3)\n",
    "plot_audio_byfeat(\n",
    "    spectral_density, abs_energy, \"spectral density\", \"abs energy\", 0.01, 4000\n",
    ")\n",
    "plt.subplot(2, 3, 4)\n",
    "plot_audio_byfeat(fft_centroid, rms, \"fft centroid\", \"rms\", 150000, 0.15)\n",
    "plt.subplot(2, 3, 5)\n",
    "plot_audio_byfeat(fft_centroid, abs_energy, \"fft centroid\", \"abs energy\", 150000, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train-test split of features\n",
    "\n",
    "X_train, X_test = (\n",
    "    X_df[X_df.index.isin(train_ids)].squeeze(),\n",
    "    X_df[X_df.index.isin(test_ids)].squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5680459770114943\n",
      "0.5910344827586207\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587816091954023\n",
      "0.6103448275862069\n"
     ]
    }
   ],
   "source": [
    "# logistic regression with normalizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "norm = StandardScaler()\n",
    "\n",
    "lr_norm = LogisticRegression()\n",
    "lr_norm.fit(norm.fit_transform(X_train), y_train)\n",
    "print(lr_norm.score(norm.transform(X_train), y_train))\n",
    "print(lr_norm.score(norm.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like the model always predicts call :(\n",
    "np.where(not np.isclose(lr_norm.predict_proba(X_test), [1.0, 0.0]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.530898997935346 15.792923367532413\n"
     ]
    }
   ],
   "source": [
    "# compare loss and error with baseline\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "train_pred = lr_norm.predict_proba(X_train)[:, 0]\n",
    "train_loss = log_loss(y_train, train_pred)\n",
    "\n",
    "test_pred = lr_norm.predict_proba(X_test)[:, 0]\n",
    "test_loss = log_loss(y_test, test_pred)\n",
    "# logreg_error = (lr_norm.predict(norm.transform(X_test)) != y_test).mean()  # mean error = 1-accuracy\n",
    "\n",
    "# baseline_pred = np.repeat(1, y_test.size) # always guess 1\n",
    "baseline_pred = np.repeat(y_train.mean(), y_test.size)  # guess the mean\n",
    "baseline_loss = log_loss(y_test, baseline_pred)\n",
    "baseline_error = (1 != y_test).mean()\n",
    "\n",
    "print(train_loss, test_loss)\n",
    "# print(baseline_loss, baseline_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5503448275862068\n",
      "0.5427586206896552\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean())\n",
    "print(y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full workflow using random smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_split_ids(percent, train_ids, test_ids):\n",
    "    # random subset of train\n",
    "    train_rand_ids = np.random.choice(\n",
    "        train_ids, size=int(len(train_ids) * percent), replace=False\n",
    "    )\n",
    "    # random subset of test\n",
    "    test_rand_ids = np.random.choice(\n",
    "        test_ids, size=int(len(test_ids) * percent), replace=False\n",
    "    )\n",
    "    return train_rand_ids, test_rand_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and compare multiple featurization (feature calculator) options\n",
    "# useful to run on a subset of the data\n",
    "\n",
    "\n",
    "def test_multiple_featurizations(train_rand_ids, test_rand_ids, *featurization_lst):\n",
    "    all_rand_ids = np.concatenate((train_rand_ids, test_rand_ids))\n",
    "\n",
    "    # lists across feature calcs:\n",
    "    X_rand_lst = []  # df of features\n",
    "    X_train_rand_lst = []  # df of features for train\n",
    "    X_test_rand_lst = []  # df of features for test\n",
    "    models_lst = []  # lr models\n",
    "    scores_lst = []\n",
    "\n",
    "    # make train-test splits of labels\n",
    "    y_train_rand, y_test_rand = (\n",
    "        y_df[y_df.index.isin(train_rand_ids)].drop(columns=[\"id\"]).squeeze(),\n",
    "        y_df[y_df.index.isin(test_rand_ids)].drop(columns=[\"id\"]).squeeze(),\n",
    "    )\n",
    "\n",
    "    # run through full workflow for each feature calculator\n",
    "    for featurization in featurization_lst:\n",
    "        # featurize data from given ids\n",
    "        X_rand = featurize_set(all_rand_ids, featurization)\n",
    "        X_rand_lst.append(X_rand)\n",
    "\n",
    "        # make train-test splits of featurized data\n",
    "        X_train_rand, X_test_rand = (\n",
    "            X_rand[X_rand.index.isin(train_rand_ids)].squeeze(),\n",
    "            X_rand[X_rand.index.isin(test_rand_ids)].squeeze(),\n",
    "        )\n",
    "        # Why doesn't X_rand.loc[train_rand_ids,:] work? (wrong # rows)\n",
    "        X_train_rand_lst.append(X_test_rand)\n",
    "        X_test_rand_lst.append(X_train_rand)\n",
    "\n",
    "        # train models\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(X_train_rand, y_train_rand)\n",
    "        models_lst.append(lr)\n",
    "\n",
    "        # score models\n",
    "        score = lr.score(X_test_rand, y_test_rand)\n",
    "        scores_lst.append(score)\n",
    "\n",
    "    return X_rand_lst, models_lst, scores_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random ids from 5% of dataset\n",
    "train_rand_ids, test_rand_ids = get_rand_split_ids(0.05, train_ids, test_ids)\n",
    "\n",
    "# make train-test splits of labels\n",
    "y_train_rand, y_test_rand = (\n",
    "    y_df[y_df.index.isin(train_rand_ids)].drop(columns=[\"id\"]).squeeze(),\n",
    "    y_df[y_df.index.isin(test_rand_ids)].drop(columns=[\"id\"]).squeeze(),\n",
    ")\n",
    "\n",
    "# check that we have a good distribution of value counts\n",
    "print(y_train_rand.value_counts())\n",
    "print(y_test_rand.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_rand_ids))\n",
    "print(len(y_train_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to extract (options)\n",
    "\n",
    "# presets can be found here: https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.settings.ComprehensiveFCParameters\n",
    "# with more details here: https://tsfresh.readthedocs.io/en/latest/_modules/tsfresh/feature_extraction/settings.html#MinimalFCParameters\n",
    "# manual_fc_params\n",
    "# selected_fc_params\n",
    "# EfficientFCParameters()\n",
    "# ComprehensiveFCParameters()\n",
    "# MinimalFCParameters()\n",
    "\n",
    "\n",
    "manual_fc_params_A = {\n",
    "    \"abs_energy\": None,\n",
    "    \"fft_aggregated\": [{\"aggtype\": \"centroid\"}, {\"aggtype\": \"kurtosis\"}],\n",
    "    \"root_mean_square\": None,\n",
    "    \"spkt_welch_density\": [{\"coeff\": 2}, {\"coeff\": 5}, {\"coeff\": 8}],\n",
    "}\n",
    "\n",
    "# manual_fc_params_B = {\n",
    "#                         \"abs_energy\": None,\n",
    "#                         \"fft_aggregated\": [{\"aggtype\":\"centroid\"}],\n",
    "#                         \"root_mean_square\": None,\n",
    "#                         \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}]\n",
    "# }\n",
    "# manual_fc_params_C = {\n",
    "#                         \"abs_energy\": None,\n",
    "#                         \"fft_aggregated\": [{\"aggtype\":\"centroid\"}, {\"aggtype\":\"kurtosis\"}],\n",
    "#                         \"root_mean_square\": None,\n",
    "#                         \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}],\n",
    "#                         \"number_crossing_m\": [{\"m\":0}]\n",
    "# }\n",
    "\n",
    "\n",
    "# manual_fc_params_B = {\n",
    "#                         \"abs_energy\": None,\n",
    "#                         \"fft_aggregated\": [{\"aggtype\":\"centroid\"}, {\"aggtype\":\"kurtosis\"}, {\"aggtype\":\"skew\"}, {\"aggtype\":\"variance\"}],\n",
    "#                         \"root_mean_square\": None,\n",
    "#                         \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 3 mins for manual, 5% of dataset\n",
    "# takes 168.2 s for minimal, 5% of dataset\n",
    "# takes 169s for selected (3 features), 5% of dataset\n",
    "# takes 542s (9m) for all 3, 5% of dataset\n",
    "# takes 13h for efficient fc, 0.5% of dataset\n",
    "\n",
    "X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(\n",
    "    train_rand_ids,\n",
    "    test_rand_ids,\n",
    "    MinimalFCParameters(),\n",
    "    selected_fc_params,\n",
    "    manual_fc_params,\n",
    ")\n",
    "# X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(train_rand_ids, test_rand_ids, manual_fc_params_A, manual_fc_params_B, manual_fc_params_C)\n",
    "# X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(train_rand_ids, test_rand_ids, manual_fc_params_B)\n",
    "# X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(train_rand_ids, test_rand_ids, EfficientFCParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in scores_lst:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_preset_sel_manual = {\n",
    "    \"X\": X_rand_lst.copy(),\n",
    "    \"models\": models_lst.copy(),\n",
    "    \"scores\": scores_lst.copy(),\n",
    "}\n",
    "# compare_manual = {'X': X_rand_ls.copy(), 'models': models_lst.copy(), 'scores': scores_lst.copy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select good features from the first feature calculator list\n",
    "# make train-test splits of featurized data\n",
    "X_for_selection = X_rand_lst[0]\n",
    "X_train_for_selection, X_test_for_selection = (\n",
    "    X_for_selection[X_for_selection.index.isin(train_rand_ids)].squeeze(),\n",
    "    X_for_selection[X_for_selection.index.isin(test_rand_ids)].squeeze(),\n",
    ")\n",
    "\n",
    "X_selected = select_features(X_train_for_selection, y_train_rand)\n",
    "\n",
    "# get a dictionary of the good features parameters, to use again later\n",
    "kind_to_fc_parameters = from_columns(X_selected)\n",
    "# new_selected_fc_params = kind_to_fc_parameters['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in compare_preset_sel_manual[\"scores\"]:\n",
    "    print(score)\n",
    "# for score in compare_manual['scores']:\n",
    "#     print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python385jvsc74a57bd09d6766ad3736c29ebfe40ecf2d41a2944950e1cce237755c2a58ee0718f8bfc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "9d6766ad3736c29ebfe40ecf2d41a2944950e1cce237755c2a58ee0718f8bfc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}