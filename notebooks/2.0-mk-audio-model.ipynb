{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature\n",
    "# from pydub import AudioSegment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction.settings import from_columns\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import pickle\n",
    "\n",
    "ROOT_PATH = Path(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ids, labels, and train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ROOT_PATH / \"data/raw/metadata.csv\")\n",
    "# svc - song vs call ids\n",
    "# filter ids -> <20s, quality A & B\n",
    "# svc ids -> only rows that have call or song (not both)\n",
    "filter_ids = pd.read_json(ROOT_PATH / \"data/raw/filter_ids.json\").squeeze()\n",
    "svc_ids = pd.read_json(ROOT_PATH / \"data/raw/song_vs_call.json\").squeeze()\n",
    "svc_df = df.loc[df.id.isin(svc_ids)].copy()\n",
    "# set index to id\n",
    "svc_df.set_index('id', inplace=True)\n",
    "\n",
    "with open(ROOT_PATH / \"data/processed/svc_split.json\") as svc_split_file:\n",
    "    svc_split = json.load(svc_split_file)\n",
    "    train_ids = svc_split[\"train_ids\"]\n",
    "    test_ids = svc_split[\"test_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add response variable\n",
    "type_col = svc_df.type.str.lower().str.replace(\" \", \"\").str.split(\",\")\n",
    "filtered_type_col = type_col.apply(lambda l: set(l) - {\"call\", \"song\"})\n",
    "svc_df[\"pred\"] = type_col.apply(lambda l: \"call\" in l).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build y train-test\n",
    "# indexing all (svc_df and y_df) by id\n",
    "y_df = svc_df[\"pred\"]\n",
    "y_train, y_test = (\n",
    "    y_df[y_df.index.isin(train_ids)].squeeze(),\n",
    "    y_df[y_df.index.isin(test_ids)].squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Audio to Timeseries, Run High-pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply butter filter\n",
    "def highpass_filter(audio, sr):\n",
    "    # butter_coeff_b, butter_coeff_a = signal.butter(3, 1000, btype='highpass', fs=sr) # numerator and denominator\n",
    "    # butter_audio = signal.lfilter(butter_coeff_b, butter_coeff_a, audio)\n",
    "    # return butter_audio\n",
    "    return signal.lfilter(*signal.butter(3, 1000, btype='highpass', fs=sr), audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack an mp3 or wav into df of timeseries values\n",
    "def unpack_audio(id):\n",
    "    try:\n",
    "        audio_path = ROOT_PATH / (\"data/raw/recordings/\" + str(id) + \".mp3\")\n",
    "        # load mp3 as audio timeseries arr\n",
    "        timeseries,sr = librosa.load(audio_path)\n",
    "    except FileNotFoundError:\n",
    "        audio_path = ROOT_PATH / (\"data/raw/recordings/\" + str(id) + \".wav\")\n",
    "        timeseries,sr = librosa.load(audio_path)\n",
    "\n",
    "    # high-pass filter on audio timeseries\n",
    "    timeseries_filt = highpass_filter(timeseries,sr)\n",
    "\n",
    "    df = pd.DataFrame(timeseries_filt, columns=['val'])\n",
    "    df.reset_index(inplace=True)\n",
    "    df['id'] = id # fill col with id\n",
    "    df = df.reindex(columns=['id','index','val'])\n",
    "    df.columns = ['id','time','val']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# unpack_audio(svc_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to calculate\n",
    "# features can be found here: https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.feature_calculators.fft_aggregated\n",
    "manual_fc_params = {\n",
    "                        \"abs_energy\": None,\n",
    "                        \"fft_aggregated\": [{\"aggtype\":\"centroid\"}, {\"aggtype\":\"kurtosis\"}],\n",
    "                        \"root_mean_square\": None,\n",
    "                        \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}]\n",
    "}\n",
    "\n",
    "selected_fc_params = {\n",
    "                        'standard_deviation': None,\n",
    "                        'variance': None,\n",
    "                        'root_mean_square': None\n",
    "}\n",
    "\n",
    "def featurize_audio(id, fc_params):\n",
    "        return extract_features(unpack_audio(id), column_id='id', column_sort='time',\n",
    "                        default_fc_parameters=fc_params,\n",
    "                        disable_progressbar=False,\n",
    "                        # we impute = remove all NaN features automatically\n",
    "                        impute_function=impute,\n",
    "                        # turn off parallelization\n",
    "                        n_jobs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurize dataset\n",
    "# returns df of all combined\n",
    "def featurize_set(ids, fc_params=EfficientFCParameters()):\n",
    "    X_df = pd.DataFrame()\n",
    "    for id in tqdm(ids):\n",
    "        X_df = pd.concat([X_df,featurize_audio(id, fc_params)])\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate features\n",
    "# takes 1 hr\n",
    "# X_df = featurize_set(svc_ids, manual_fc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save calculated features\n",
    "# X_df.to_json(ROOT_PATH / f\"data/processed/audio_features_manual.json\", indent=2, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "X_df = pd.read_json(path_or_buf=ROOT_PATH / \"data/processed/audio_features.json\",orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train-test split of features\n",
    "\n",
    "X_train, X_test = (\n",
    "    X_df[X_df.index.isin(train_ids)].squeeze(),\n",
    "    X_df[X_df.index.isin(test_ids)].squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of n ids from a given list of ids\n",
    "def get_rand_ids(n, ids_lst):\n",
    "    ids_ser = pd.Series(ids_lst)\n",
    "    rand_ids = ids_ser[np.random.randint(0,len(ids_lst),size=n)].array\n",
    "    return rand_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5910344827586207\n"
     ]
    }
   ],
   "source": [
    "def get_rand_split_ids(percent, train_ids, test_ids):\n",
    "    # random subset of train\n",
    "    train_rand_ids = get_rand_ids(int(len(train_ids)*percent), train_ids)\n",
    "    # random subset of test\n",
    "    test_rand_ids = get_rand_ids(int(len(test_ids)*percent), test_ids)\n",
    "    return train_rand_ids, test_rand_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full workflow using random smaller subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features into train-test\n",
    "\n",
    "X_train_rand_minimal, X_test_rand_minimal = (\n",
    "    X_df_rand_minimal[X_df_rand_minimal.index.isin(train_rand_ids)].squeeze(),\n",
    "    X_df_rand_minimal[X_df_rand_minimal.index.isin(test_rand_ids)].squeeze(),\n",
    ")\n",
    "X_train_rand_selected, X_test_rand_selected = (\n",
    "    X_df_rand_selected[X_df_rand_selected.index.isin(train_rand_ids)].squeeze(),\n",
    "    X_df_rand_selected[X_df_rand_selected.index.isin(test_rand_ids)].squeeze(),\n",
    ")\n",
    "X_train_rand_manual, X_test_rand_manual = (\n",
    "    X_df_rand_manual[X_df_rand_manual.index.isin(train_rand_ids)].squeeze(),\n",
    "    X_df_rand_manual[X_df_rand_manual.index.isin(test_rand_ids)].squeeze(),\n",
    ")\n",
    "\n",
    "# this gives incorrect num rows :( !\n",
    "# X_train_rand = X_df_rand.loc[train_rand_ids,:]\n",
    "# X_test_rand = X_df_rand.loc[test_rand_ids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and compare multiple featurization (feature calculator) options\n",
    "# useful to run on a subset of the data\n",
    "\n",
    "# takes 3 mins for manual, 5% of dataset\n",
    "# takes 168.2 s for minimal, 5% of dataset\n",
    "# takes 169s for selected (3 features), 5% of dataset\n",
    "# takes 542s for all 3\n",
    "\n",
    "# Features to extract (options)\n",
    "# presets can be found here: https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.settings.ComprehensiveFCParameters\n",
    "# with more details here: https://tsfresh.readthedocs.io/en/latest/_modules/tsfresh/feature_extraction/settings.html#MinimalFCParameters\n",
    "# manual_fc_params\n",
    "# selected_fc_params\n",
    "# EfficientFCParameters()\n",
    "# ComprehensiveFCParameters()\n",
    "# MinimalFCParameters()\n",
    "\n",
    "def test_multiple_featurizations(train_rand_ids, test_rand_ids, *featurization_lst):\n",
    "    all_rand_ids = np.concatenate((train_rand_ids, test_rand_ids))\n",
    "\n",
    "    # lists across feature calcs:\n",
    "    X_rand_lst = [] # df of features\n",
    "    X_train_rand_lst = [] # df of features for train\n",
    "    X_test_rand_lst = [] # df of features for test\n",
    "    models_lst = [] # lr models\n",
    "    scores_lst = []\n",
    "\n",
    "    # run through full workflow for each feature calculator\n",
    "    for featurization in featurization_lst:\n",
    "        # featurize data from given ids\n",
    "        X_rand = featurize_set(all_rand_ids, featurization)\n",
    "        X_rand_lst.append(X_rand)\n",
    "\n",
    "        # make train-test splits of featurized data\n",
    "        X_train_rand, X_test_rand = (\n",
    "            X_rand[X_rand.index.isin(train_rand_ids)].squeeze(),\n",
    "            X_rand[X_rand.index.isin(test_rand_ids)].squeeze(),\n",
    "        )\n",
    "        # Why doesn't X_rand.loc[train_rand_ids,:] work? (wrong # rows)\n",
    "        X_train_rand_lst.append(X_test_rand)\n",
    "        X_test_rand_lst.append(X_train_rand)\n",
    "\n",
    "        # train models\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(X_train_rand, y_train_rand)\n",
    "        models_lst.append(lr)\n",
    "\n",
    "        # score models\n",
    "        score = lr.score(X_test_rand, y_test_rand)\n",
    "        scores_lst.append(score)\n",
    "\n",
    "    return X_rand_lst, models_lst, scores_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    13\n",
      "0     8\n",
      "Name: pred, dtype: int64\n",
      "0    5\n",
      "1    2\n",
      "Name: pred, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# generate random ids from 5% of dataset\n",
    "train_rand_ids, test_rand_ids = get_rand_split_ids(.005, train_ids, test_ids)\n",
    "\n",
    "# make train-test splits of labels\n",
    "y_train_rand, y_test_rand = (\n",
    "    y_df[y_df.index.isin(train_rand_ids)].drop(columns=[\"id\"]).squeeze(),\n",
    "    y_df[y_df.index.isin(test_rand_ids)].drop(columns=[\"id\"]).squeeze(),\n",
    ")\n",
    "\n",
    "# check that we have a good distribution of value counts\n",
    "print(y_train_rand.value_counts())\n",
    "print(y_test_rand.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_fc_params_A = {\n",
    "                        \"abs_energy\": None,\n",
    "                        \"fft_aggregated\": [{\"aggtype\":\"centroid\"}, {\"aggtype\":\"kurtosis\"}],\n",
    "                        \"root_mean_square\": None,\n",
    "                        \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}]\n",
    "}\n",
    "\n",
    "# manual_fc_params_B = {\n",
    "#                         \"abs_energy\": None,\n",
    "#                         \"fft_aggregated\": [{\"aggtype\":\"centroid\"}],\n",
    "#                         \"root_mean_square\": None,\n",
    "#                         \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}]\n",
    "# }\n",
    "# manual_fc_params_C = {\n",
    "#                         \"abs_energy\": None,\n",
    "#                         \"fft_aggregated\": [{\"aggtype\":\"centroid\"}, {\"aggtype\":\"kurtosis\"}],\n",
    "#                         \"root_mean_square\": None,\n",
    "#                         \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}],\n",
    "#                         \"number_crossing_m\": [{\"m\":0}]\n",
    "# }\n",
    "\n",
    "\n",
    "# manual_fc_params_B = {\n",
    "#                         \"abs_energy\": None,\n",
    "#                         \"fft_aggregated\": [{\"aggtype\":\"centroid\"}, {\"aggtype\":\"kurtosis\"}, {\"aggtype\":\"skew\"}, {\"aggtype\":\"variance\"}],\n",
    "#                         \"root_mean_square\": None,\n",
    "#                         \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(train_rand_ids, test_rand_ids, MinimalFCParameters(), selected_fc_params, manual_fc_params)\n",
    "# X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(train_rand_ids, test_rand_ids, manual_fc_params_A, manual_fc_params_B, manual_fc_params_C)\n",
    "# X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(train_rand_ids, test_rand_ids, manual_fc_params_B)\n",
    "X_rand_lst, models_lst, scores_lst = test_multiple_featurizations(train_rand_ids, test_rand_ids, EfficientFCParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "for score in scores_lst:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standard_deviation': None, 'variance': None, 'root_mean_square': None}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select good features from the first feature calculator list\n",
    "# make train-test splits of featurized data\n",
    "X_for_selection = X_rand_lst[0]\n",
    "X_train_for_selection, X_test_for_selection = (\n",
    "    X_for_selection[X_for_selection.index.isin(train_rand_ids)].squeeze(),\n",
    "    X_for_selection[X_for_selection.index.isin(test_rand_ids)].squeeze(),\n",
    ")\n",
    "\n",
    "X_selected = select_features(X_train_for_selection, y_train_rand)\n",
    "\n",
    "# get a dictionary of the good features parameters, to use again later\n",
    "kind_to_fc_parameters = from_columns(X_selected)\n",
    "kind_to_fc_parameters['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_PATH / \"data/processed/audio_timeseries.pkl\", \"wb+\") as pkl_file:\n",
    "    pickle.dump(audio_dict, pkl_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "for score in compare_preset_sel_manual['scores']:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3888888888888889\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python385jvsc74a57bd09d6766ad3736c29ebfe40ecf2d41a2944950e1cce237755c2a58ee0718f8bfc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "9d6766ad3736c29ebfe40ecf2d41a2944950e1cce237755c2a58ee0718f8bfc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}