{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-bb8420c6-c1fe-4532-aa2c-b93684e8d62d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we aimed to accurately classify bird sounds as songs or calls. We used 3 different approaches and models based on recording metadata, the audio data itself, and spectrogram images of the recording to perform this classification task.\n",
    "\n",
    "## Motivation\n",
    "One motivation for this project is practical and concrete, to make it faster and easier for scientists to collect data on bird populations, verify community-sourced labels, etc.\n",
    "\n",
    "The other motivation is more open-ended: one of curiosity and self-education to discover the “hidden” insights in bird sounds.  As novice bird-listeners ourselves, we are amazed by the variability and information encoded in bird calls. Bird calls reveal regional dialects, a sense of humor, information about predators in the area, indicators of ecosystem health, and other dimensions of their ecology - and inevitably also the threat posed by human activity. To our untrained ears, it's difficult to even distinguish calls from different species, let alone genders or pitch-shifts in reaction to noise pollution. Through the process visualizing and analyzing features of bird call audio, we hope we can build towards better understanding the impacts of the \"anthropony\" (sounds produced by humans) and become better listeners.\n",
    "\n",
    "## Songs vs Calls\n",
    "Bird sounds have a variety of different dimensions, but one of the first levels of categorizing bird sounds is classifying them as a song or a call, as each have distinct functions and reveal different aspects of the birds’ ecology. For example, the frequency of bird alarm calls can indicate the number of predators in an area [cite]().\n",
    "\n",
    "### Useful Terms\n",
    "**Call** : Bird vocalization\n",
    "\n",
    "**Song** : Courtship or territorial vocalization, typically from male songbirds in characteristic phrases\n",
    "\n",
    "**Note** : Smallest unit of vocalization, an uninterrupted trace on the spectrogram\n",
    "\n",
    "**Syllable** : Combination of notes that are separated by short intervals; syllables are separated by longer intervals\n",
    "\n",
    "**Rhythmic structure** : The way notes, syllables, or calls repeat (count and rate of repetition)\n",
    "\n",
    "**Fundamental frequency** : The call pitch, visible on a spectrogram usually as the lowest band in a stack of integer-multiple bands (harmonics)\n",
    "\n",
    "**Harmonics** : Integer multiple frequency bands upon the fundamental frequency band.\n",
    "\n",
    "**Frequency modulation** : Changes in fundamental frequency contour during a call\n",
    "\n",
    "**Tonal call** : Call, containing the fundamental frequency and its related harmonics.\n",
    "\n",
    "**Noisy call** : Call where the fundamental frequency and harmonics are indistinguishable or lacking from the inside. Looks like uniform noise on the spectrogram.\n",
    "\n",
    "**Dominant frequency** : The frequency where the maximum energy of a call is concentrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-0e69944b-7b03-4fc1-afaf-fb18c9b9839b",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Related Work on Bird Sound Data\n",
    "## Gender identification using acoustic analysis in birds without external sexual dimorphism\n",
    "> Volodin, I.A., Volodina, E.V., Klenova, A.V. et al. Gender identification using acoustic analysis in birds without external sexual dimorphism. Avian Res 6, 20 (2015). https://doi.org/10.1186/s40657-015-0033-y\n",
    ", [Article Link](https://avianres.biomedcentral.com/articles/10.1186/s40657-015-0033-y)\n",
    "\n",
    "\n",
    "### Background\n",
    "- Determining the sex of adult birds using their calls is useful for monomorphic birds (no difference in appearance across sexes), is noninvasive, and may be done at a distance\n",
    "- It is important to sex birds for wildlife management - breeding and census estimates\n",
    "- Bird calls likely differ across sexes due to differences in morphology (e.g. size of beak, vocal organ, trachea), the method of producing a call, or in the type of calls/songs\n",
    "\n",
    "### Contribution\n",
    "- Researchers used spectrograms and power spectra\n",
    "![spectrograms](https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40657-015-0033-y/MediaObjects/40657_2015_33_Fig1_HTML.gif?as=webp)\n",
    "- They used simple visual inspection (without measuring acoustic variables) for certain species, or using Discriminate Function Analysis when acoustic variables overlapped more across sexes.\n",
    "    - DFA accuracy > 70% was considered sufficient but low reliability for sexing by call (50% would be equivalent to random chance)\n",
    "    - 91 - 100% : perfect reliability\n",
    "- Relevant vocal features for differentiating sex were:\n",
    "    - Average fundamental frequency\n",
    "    - Maximum fundamental frequency\n",
    "    - Biphonation (2 independent fundamental frequencies)\n",
    "    - Duration of notes\n",
    "    - Number of syllables in call\n",
    "    - Amplitude modulation (wideband spectra)\n",
    "    - Intervals between syallables\n",
    "\n",
    "## Further Reading\n",
    "- [2009 Classifying Bird Calls with Supervised Learning](https://eecs.oregonstate.edu/research/bioacoustics/briggs_icdm09.pdf) - useful background on audio features\n",
    "- [How Birds Develop Song Dialects](https://sora.unm.edu/sites/default/files/journals/condor/v077n04/p0385-p0406.pdf) - regional dialects in birdsong\n",
    "- [Do bird calls of the same species differ across countries?](https://www.researchgate.net/post/Does-bird-calls-of-same-species-from-one-country-differ-from-the-calls-from-other-country) - Researchgate forum, some more links here\n",
    "\n",
    "## Regional dialects have been discovered among many bird species and the Yellowhammer is a great example\n",
    "\n",
    "[Current Project](http://www.yellowhammers.net/about)\n",
    "\n",
    "Birds have been observed to have regional dialects, as a result of ecological drivers, [human disturbances](https://academic.oup.com/beheco/article/30/6/1501/5526711?login=true), and cultural evolution.\n",
    "\n",
    "A famous citizen science project took place in the Czech Republic starting in 2011 to study dialects of Yellowhammer birds (a species easily recognized by sight and sound). Its dialects differ in the frequency and length of their final syllables.\n",
    "\n",
    "It revealed two main dialect groups and the border between, and brought up more questions like: How are the dialects maintained? What causes the dialect boundaries between neighboring habitats? How do dialects evolve?\n",
    "![yellowhammer dialects](https://bou.org.uk/wp-content/uploads/2013/05/Pipek-fig2-530x296.png)\n",
    "\n",
    "## Other bird audio data science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-2e28dd94-127d-483c-9d7d-dc9b6fe29aaf",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Collecting Data\n",
    "For our analysis, we used audio files and metadata from xeno-canto.org. Xeno-canto (XC) is a website for collecting and sharing audio recordings of birds. \n",
    "\n",
    "Recordings and identifications on XC are sourced from the community (anyone can join), as are recording quality ratings and flags for wrong IDs.\n",
    "\n",
    "XC has a [very simple API](https://www.xeno-canto.org/explore/api) that allows us to make RESTful queries, without even requiring an API key. A request url looks like this: `https://www.xeno-canto.org/api/2/recordings?query=bearded+bellbird+q:A&page=5`.\n",
    "We can also test our request by going to `https://www.xeno-canto.org/explore?query=[our-query-here]`. You can learn more about query parameters [here](https://www.xeno-canto.org/help/search).\n",
    "\n",
    "Request payloads contain fields that are nearly identical to the query parameters. Some especially important fields were:\n",
    "- **gen**  : genus\n",
    "- **sp**   : species\n",
    "- **type** : a comma-separated list of the sound types of the recording (e.g. call, song, male, female)\n",
    "- **file** : the url to the audio file\n",
    "- **sono** : the urls to the sonogram image files\n",
    "- **loc** : the location of the recording (e.g. Pittsburgh, Allegheny County, Pennsylvania)\n",
    "- **lat** & **lng** : the latitude and longitude of the recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring & Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Classification Model\n",
    "In one model, we used the tabular metadata from xeno-canto (XC) entries, with some filtering and pre-processing, to train a Logistic Regression model.\n",
    "\n",
    "We used the genus, species, English name, and location (latitude and longitude) from XC metadata, all mapped and imputed using sk-learn transformers to one-hot encoders apart from latitude and longitude (mapped using min-max scaling). We also extracted additional features of identified gender (male or female) and age of the bird (juvenile or adult) from the \"type\" notes in XC metadata.\n",
    "\n",
    "![image of metadata df](https://github.com/adithyabsk/pracds_final/blob/main/notebooks/assets/metadata_Xdf.png)\n",
    "\n",
    "## Model Performance\n",
    "score of .724"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification Model\n",
    "For one of our models, we used the bird audio recordings themselves (mp3 and wav files). We converted the audio files into timeseries data, then used ts-fresh to extract time and frequency-domain features, which we used to train a Logistic Regression model.\n",
    "\n",
    "You can refer to [2.0-mk-audio-model.ipynb](https://github.com/adithyabsk/pracds_final/blob/main/notebooks/2.0-mk-audio-model.ipynb) or the [python scripts](https://github.com/adithyabsk/pracds_final/blob/main/pracds_final/features/proc_audio.py) in our dvc pipeline for details on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Audio Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio to Timeseries\n",
    "Using the [librosa](https://librosa.org/) package, we loaded mp3s and wav files into arrays of timeseries data. The `librosa.load()` function is not very efficient in loading mp3 files, as it falls back to using the audioread and ffmpeg libraries (see [here](https://www.audiolabs-erlangen.de/resources/MIR/FMP/B/B_PythonAudio.html) and [here](https://stackoverflow.com/questions/59854527/librosa-load-takes-too-long-to-loadsample-mp3-files)), so it may be worth looking into alternatives like [pydub](https://github.com/jiaaro/pydub)'s `AudioSegment()` for further work with audio analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "Once the audio data was in an array format, we ran it through a high-pass [Butterworth filter](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html) to take out background noise. We tested different order and critical frequency parameters for Butterworth and Firwin filters, then plotted the spectrograms and listened to the audio before and after filtering to determine which parameters and filters best reduced background noise without clipping the bird sound frequencies. You can see [1.0-mk-audio-exploration.ipynb](https://github.com/adithyabsk/pracds_final/blob/main/notebooks/1.0-mk-audio-exploration.ipynb) for details on this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of filter comparison](https://github.com/adithyabsk/pracds_final/blob/main/notebooks/assets/audio_comparing_filters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load audio and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection & Extraction\n",
    "From the filtered audio data arrays, we used [tsfresh](https://tsfresh.readthedocs.io/en/latest/index.html) to extract audio features. tsfresh takes in dataframes with an id column, time column, and value column. We extracted features from one audio id at a time, so each of our dataframes' id columns were filled with a single id at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of time series input df for a single id]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: we found it was best to immediately featurize each audio file after unpacking it into a timeseries array. The timeseries arrays are very large, so trying to store them for all audio files actually crashed our notebook and made us run out of memory! (We estimated it would be a dataframe of about 150 million rows to store all 5800 audio files as arrays.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tsfresh has built-in feature calculator presets to extract certain features all at once, and also allows you to manually specify which features to extract with a dictionary parameter. In addition, it provides a useful `select_features()` function that checks the significance of each of the columns in feature matrix X.\n",
    "\n",
    "It takes a very long time to extract the `ComprehensiveFCParameters()` and `EfficientFCParameters()` (est. 39 and 13 hours for just 5% of the dataset), so we manually specified the following small set of features based on our domain background research and understanding of audio analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_fc_params = {\n",
    "    \"abs_energy\": None,\n",
    "    \"fft_aggregated\": [{\"aggtype\":\"centroid\"}, {\"aggtype\":\"kurtosis\"}],\n",
    "    \"root_mean_square\": None,\n",
    "    \"spkt_welch_density\": [{\"coeff\":2},{\"coeff\":5},{\"coeff\":8}]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of feature output df for all ids]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning & Performance\n",
    "To begin tuning our model, we compared different feature selections using a random subset of the training and testing data. We extracted features using our manual selection, tsfresh's `MinimalFCParameters()`, and the features selected from the MinimalFCParameters() set on 5% of the dataset, and found the scores were around .58, .47, and .47 respectively. Because our manual selection performed best, and because the process of extracting many features at once then calculating significance was very time consuming, we continued with the manual selection approach.\n",
    "\n",
    "We tested a few other manually selected features, including different fft aggregation types and zero-crossing rate, but found that they did not improve model performance. With more time, it would be worth exploring more features based on domain knowledge.\n",
    "\n",
    "We split our full feature matrix X into training and testing sets (matching those used in the other models), then trained a Logistic Regression model from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) using both the normalized and unnormalized features.\n",
    "\n",
    "We can compare our model's performance to a baseline of guessing the mean of the training labels. We can also think of this as the % of the training data that is labelled 1, meaning the bird sound is a call and not a song, which is 55%. Our Logistic Regression model performs slightly above the baseline. By plotting some of our features against each other, we can already see that it is nearly impossible for us to distinguish song and call points in the feature space from a human perspective. We hope with more time spent on tuning feature selection (perhaps exploring a wider range of possible features like fundamental frequency using [tsfel](https://tsfel.readthedocs.io/en/latest/descriptions/feature_list.html)), feature parameters, and possibly feature scope (frame size) we can make our model more accurate.\n",
    "\n",
    "| Model      | Score      | LogLoss | Error    |\n",
    "| ---------- | ---------- | ------- | -------- |\n",
    "| LogReg     | .61        | 18.75   | .39      |\n",
    "| Baseline   | .55        | .69     | .46      |\n",
    "\n",
    "![image of feat plots](https://github.com/adithyabsk/pracds_final/blob/main/notebooks/assets/audio_feature_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Classification Model\n",
    "\n",
    "We used a computer vision approach to analyze spectrograms by training a fastai CNN, based on the pre-trained model xresnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "- this classification could be the first level in a hierarchy of models / data analysis\n",
    "- once you know it's call vs song, you can analyze it in a more specific way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00006-9e03cc75-5c5b-4394-8935-77ab257fa849",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 109,
    "execution_start": 1620504813660,
    "source_hash": "c400395a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 1,921\n",
      "Line Count: 14\n"
     ]
    }
   ],
   "source": [
    "from nbformat import read\n",
    "\n",
    "# Default path for this notebook (can be run inside the notebook)\n",
    "path = \"0.0-mk-final-report.ipynb\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = read(f, 4)\n",
    "\n",
    "# Count the number of lines in markdown or heading cells\n",
    "word_count = sum(\n",
    "    [\n",
    "        len(cell[\"source\"].replace(\"#\", \"\").lstrip().split(\" \"))\n",
    "        for cell in nb[\"cells\"]\n",
    "        if cell.cell_type in [\"markdown\", \"heading\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Count number of lines in the notebook and subtract the number of\n",
    "# lines in this cell\n",
    "line_count = (\n",
    "    sum(\n",
    "        [\n",
    "            # Filter out cells that are comments or are empty\n",
    "            len(\n",
    "                list(\n",
    "                    filter(\n",
    "                        lambda line: not (line.lstrip().startswith(\"#\")),\n",
    "                        cell[\"source\"].split(\"\\n\"),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for cell in nb[\"cells\"]\n",
    "            if cell.cell_type == \"code\"\n",
    "        ]\n",
    "    )\n",
    "    - 27\n",
    ")\n",
    "\n",
    "print(f\"Word Count: {word_count:,}\")\n",
    "print(f\"Line Count: {line_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "bf38dd1c-03b4-4794-8429-f8b5a06d2638",
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python385jvsc74a57bd09d6766ad3736c29ebfe40ecf2d41a2944950e1cce237755c2a58ee0718f8bfc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
